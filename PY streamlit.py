import streamlit as st
import os
import pdfplumber
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

# --- 专转 API ---
# 拽 :   注 砖 -Secrets,  砖 砖转砖 驻转 砖
MY_NEW_KEY = "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"

try:
    if "OPENAI_API_KEY" in st.secrets:
        api_key = st.secrets["OPENAI_API_KEY"]
    else:
        api_key = MY_NEW_KEY
except:
    api_key = MY_NEW_KEY

os.environ["OPENAI_API_KEY"] = api_key

# --- 专转 祝 ---
st.set_page_config(page_title="注专 -PDF ", page_icon="", layout="centered")

# 注爪 住住 转 注专转 (RTL)
st.markdown("""
    <style>
    .stApp {
        direction: RTL;
        text-align: right;
    }
    div[data-testid="stChatMessageContent"] {
        text-align: right;
    }
    </style>
    """, unsafe_allow_html=True)

st.title(" 爪' 注 住 -PDF 砖")
st.subheader("注专转 RAG 驻砖 转 住")

# --- 专转 转 ---
# 注,  住转 注 转拽转 -DB 砖注转 -GitHub
db_path = "vectorstore_db"

# 驻拽爪 注 砖 住住 转
@st.cache_resource
def get_vector_db():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # 注转 住住 转 拽 转拽 砖注转 -GitHub
    if os.path.exists(db_path):
        return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    else:
        st.error("住住 转 (vectorstore_db)  爪 -GitHub.  砖注转 转 转拽.")
        st.stop()

# 转 注专转
with st.spinner("转 转 住住 转..."):
    vector_db = get_vector_db()

# ---  砖 ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# 爪转 住专转 注转
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 拽 砖转砖
if prompt := st.chat_input("砖 转 砖 注 住..."):
    # 住驻转 注转 砖转砖
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 爪专转 转砖
    with st.chat_message("assistant"):
        with st.spinner("注 转砖..."):
            # 驻砖 住
            docs = vector_db.similarity_search(prompt, k=5)
            
            context_list = []
            for d in docs:
                src = d.metadata.get('source', '拽专  注')
                context_list.append(f"--- 拽专: {src} ---\n{d.page_content}")
            
            context = "\n\n".join(context_list)
            
            # 拽专 -LLM
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            
            full_prompt = f"""
            转 注专  拽爪注. 
            1.  砖转砖 砖 砖转 住, 注  住.
            2.  砖转砖 砖 砖 拽爪注转, 注 驻专 注专转 专 注 住 拽砖专 (Context) 爪专祝 .
            3.  转砖  驻注 住, 爪 转 住.
            4. 住祝  转砖 拽爪注转, 爪 驻专  拽爪 拽专 拽 注.

            Context:
            {context}

            Question:
            {prompt}
            """
            
            try:
                response = llm.invoke(full_prompt).content
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"砖 转拽砖专转 注 : {e}")
