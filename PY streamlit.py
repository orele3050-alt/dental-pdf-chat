import streamlit as st
import os
import pdfplumber
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

# --- ×”×’×“×¨×•×ª API ---
# ×”×—×œ×£ ××ª ×”-API_KEY ×‘××¤×ª×— ×”×××™×ª×™ ×©×œ×š (sk-...)
os.environ["OPENAI_API_KEY"] = "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"

# --- ×”×’×“×¨×•×ª ×“×£ ---
st.set_page_config(page_title="×¢×•×–×¨ ×”-PDF ×”×—×›×", page_icon="ğŸ“š", layout="centered")

# ×¢×™×¦×•×‘ ×‘×¡×™×¡×™ ×œ×ª××™×›×” ×‘×¢×‘×¨×™×ª (RTL)
st.markdown("""
    <style>
    .stApp {
        direction: RTL;
        text-align: right;
    }
    div[data-testid="stChatMessageContent"] {
        text-align: right;
    }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ“š ×¦'××˜ ×¢× ××¡××›×™ ×”-PDF ×©×œ×š")
st.subheader("××¢×¨×›×ª RAG ×œ×—×™×¤×•×© ×•× ×™×ª×•×— ××¡××›×™×")

# --- ×”×’×“×¨×•×ª × ×ª×™×‘×™× ---
folder_path = r"C:\Users\elnatan_u\Downloads\drive-download-20251221T143827Z-1-001"
db_path = "vectorstore_db"

# ×¤×•× ×§×¦×™×” ×œ×˜×¢×™× ×” ××• ×‘× ×™×™×” ×©×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™×
@st.cache_resource
def get_vector_db():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # ×˜×¢×™× ×ª ×‘×¡×™×¡ × ×ª×•× ×™× ×§×™×™× ×× ×™×©
    if os.path.exists(db_path):
        return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    
    # ×¡×¨×™×§×ª ×”×ª×™×§×™×™×” ×‘××™×“×” ×•×œ× ×§×™×™× ××™× ×“×§×¡
    if not os.path.exists(folder_path):
        st.error(f"×”×ª×™×§×™×™×” ×œ× × ××¦××” ×‘× ×ª×™×‘: {folder_path}")
        st.stop()
        
    docs = []
    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(".pdf")]
    
    if not pdf_files:
        st.error("×œ× × ××¦××• ×§×‘×¦×™ PDF ×‘×ª×™×§×™×™×”.")
        st.stop()

    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, filename in enumerate(pdf_files):
        status_text.text(f"××¢×‘×“ ×§×•×‘×¥ {i+1} ××ª×•×š {len(pdf_files)}: {filename}")
        full_path = os.path.join(folder_path, filename)
        try:
            with pdfplumber.open(full_path) as pdf:
                text = ""
                for page in pdf.pages:
                    page_content = page.extract_text()
                    if page_content:
                        text += page_content + "\n"
            
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
            chunks = text_splitter.split_text(text)
            
            for chunk in chunks:
                docs.append(Document(page_content=chunk, metadata={"source": filename}))
        except Exception as e:
            st.warning(f"×©×’×™××” ×‘×¢×™×‘×•×“ {filename}: {e}")
        
        progress_bar.progress((i + 1) / len(pdf_files))

    if not docs:
        st.error("×œ× ×”×¦×œ×—×ª×™ ×œ×—×œ×¥ ×˜×§×¡×˜ ××”×§×‘×¦×™×.")
        st.stop()

    vectorstore = FAISS.from_documents(docs, embeddings)
    vectorstore.save_local(db_path)
    status_text.text("âœ… ×‘×¡×™×¡ ×”× ×ª×•× ×™× ××•×›×Ÿ ×œ×©×™××•×©!")
    return vectorstore

# ××ª×—×•×œ ×”××¢×¨×›×ª
with st.spinner("×××ª×—×œ ××ª ×‘×¡×™×¡ ×”× ×ª×•× ×™×..."):
    vector_db = get_vector_db()

# --- × ×™×”×•×œ ×©×™×—×” ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# ×”×¦×’×ª ×”×™×¡×˜×•×¨×™×™×ª ×”×”×•×“×¢×•×ª
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ×§×œ×˜ ××”××©×ª××©
if prompt := st.chat_input("×©××œ ××•×ª×™ ××©×”×• ×¢×œ ×”××¡××›×™×..."):
    # ×”×•×¡×¤×ª ×”×•×“×¢×ª ×”××©×ª××©
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ×™×¦×™×¨×ª ×ª×©×•×‘×”
    with st.chat_message("assistant"):
        with st.spinner("××¢×‘×“ ×ª×©×•×‘×”..."):
            # ×—×™×¤×•×© ×‘××¡××›×™× - ×©×œ×™×¤×ª 5 ×”×§×˜×¢×™× ×”×›×™ ×¨×œ×•×•× ×˜×™×™×
            docs = vector_db.similarity_search(prompt, k=5)
            
            context_list = []
            for d in docs:
                src = d.metadata.get('source', '××§×•×¨ ×œ× ×™×“×•×¢')
                context_list.append(f"--- ××§×•×¨: {src} ---\n{d.page_content}")
            
            context = "\n\n".join(context_list)
            
            # ×§×¨×™××” ×œ-LLM ×¢× ×”-Prompt ×”××©×•×¤×¨
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            
            full_prompt = f"""
            ××ª×” ×¢×•×–×¨ ××™× ×˜×œ×™×’× ×˜×™ ×•××§×¦×•×¢×™. 
            1. ×× ×”××©×ª××© ×©×•××œ ×©××œ×ª × ×™××•×¡×™×Ÿ (×›××• ×”×™×™, ××” ×§×•×¨×”?, ××™ ××ª×”?), ×¢× ×” ×œ×• ×‘× ×™××•×¡ ×•×”×¡×‘×¨ ×©××ª×” ×›××Ÿ ×›×“×™ ×œ×¢×–×•×¨ ×œ×• ×œ× ×ª×— ××ª ×”××¡××›×™× ×”××§×¦×•×¢×™×™× ×©×œ×•.
            2. ×× ×”××©×ª××© ×©×•××œ ×©××œ×” ××§×¦×•×¢×™×ª, ×¢× ×” ×‘×¤×™×¨×•×˜ ×•×‘×¢×‘×¨×™×ª ×¨×”×•×˜×” ×¢×œ ×¡××š ×”×”×§×©×¨ (Context) ×”××¦×•×¨×£ ×‘×œ×‘×“.
            3. ×× ×”×ª×©×•×‘×” ××™× ×” ××•×¤×™×¢×” ×‘××¡××›×™×, ×¦×™×™×Ÿ ×–××ª ×‘× ×™××•×¡.
            4. ×‘×¡×•×£ ×›×œ ×ª×©×•×‘×” ××§×¦×•×¢×™×ª ×©××¡×ª××›×ª ×¢×œ ×”××¡××›×™×, ×¦×™×™×Ÿ ×‘×¤×™×¨×•×˜ ×××™×œ×• ×§×‘×¦×™ ××§×•×¨ × ×œ×§×— ×”××™×“×¢.

            Context:
            {context}

            Question:
            {prompt}
            """
            
            try:
                response = llm.invoke(full_prompt).content
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"×©×’×™××” ×‘×ª×§×©×•×¨×ª ×¢× ×”××•×“×œ: {e}")